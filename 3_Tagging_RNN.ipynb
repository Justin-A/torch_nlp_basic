{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "\n",
    "def load_dataset(test_sen=None):    \n",
    "    tokenize = lambda x: x.split()\n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=200)\n",
    "    LABEL = data.LabelField()\n",
    "    train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "    TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300))\n",
    "    LABEL.build_vocab(train_data)\n",
    "\n",
    "    word_embeddings = TEXT.vocab.vectors\n",
    "    print (\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n",
    "    print (\"Vector size of Text Vocabulary: \", TEXT.vocab.vectors.size())\n",
    "    print (\"Label Length: \" + str(len(LABEL.vocab)))\n",
    "\n",
    "    train_data, valid_data = train_data.split()\n",
    "    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data), batch_size=32, sort_key=lambda x: len(x.text), repeat=False, shuffle=True)\n",
    "\n",
    "    vocab_size = len(TEXT.vocab)\n",
    "\n",
    "    return TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weight = nn.Parameter(weights, requires_grad=False)\n",
    "        self.rnn = nn.RNN(embedding_length, hidden_size, num_layers=2, bidirectional=True)\n",
    "        self.label = nn.Linear(4*hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_sentences, batch_size=None):\n",
    "        input = self.word_embeddings(input_sentences)\n",
    "        input = input.permute(1, 0, 2)\n",
    "        \n",
    "        if batch_size is None:\n",
    "            h_0 = Variable(torch.zeros(4, self.batch_size, self.hidden_size).cuda())\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(4, batch_size, self.hidden_size).cuda())\n",
    "        \n",
    "        output, h_n = self.rnn(input, h_0)\n",
    "        h_n = h_n.permute(1, 0, 2)\n",
    "        h_n = h_n.contiguous().view(h_n.size()[0], h_n.size()[1]*h_n.size()[2])\n",
    "        logits = self.label(h_n)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Vocabulary: 251639\n",
      "Vector size of Text Vocabulary:  torch.Size([251639, 300])\n",
      "Label Length: 2\n",
      "Epoch: 1, Idx: 100, Training Loss: 0.6538, Training Accuracy:  59.38%\n",
      "Epoch: 1, Idx: 200, Training Loss: 0.7323, Training Accuracy:  28.12%\n",
      "Epoch: 1, Idx: 300, Training Loss: 0.6863, Training Accuracy:  62.50%\n",
      "Epoch: 1, Idx: 400, Training Loss: 0.6000, Training Accuracy:  68.75%\n",
      "Epoch: 1, Idx: 500, Training Loss: 0.6716, Training Accuracy:  65.62%\n",
      "Epoch: 01, Train Loss: 0.701, Train Acc: 56.00%, Val. Loss: 0.684682, Val. Acc: 53.44%\n",
      "Epoch: 2, Idx: 100, Training Loss: 0.6071, Training Accuracy:  62.50%\n",
      "Epoch: 2, Idx: 200, Training Loss: 0.6640, Training Accuracy:  53.12%\n",
      "Epoch: 2, Idx: 300, Training Loss: 0.7665, Training Accuracy:  43.75%\n",
      "Epoch: 2, Idx: 400, Training Loss: 0.7446, Training Accuracy:  65.62%\n",
      "Epoch: 2, Idx: 500, Training Loss: 0.7483, Training Accuracy:  40.62%\n",
      "Epoch: 02, Train Loss: 0.702, Train Acc: 55.19%, Val. Loss: 0.726367, Val. Acc: 53.12%\n",
      "Epoch: 3, Idx: 100, Training Loss: 0.6248, Training Accuracy:  68.75%\n",
      "Epoch: 3, Idx: 200, Training Loss: 0.7327, Training Accuracy:  50.00%\n",
      "Epoch: 3, Idx: 300, Training Loss: 0.6317, Training Accuracy:  65.62%\n",
      "Epoch: 3, Idx: 400, Training Loss: 0.5729, Training Accuracy:  71.88%\n",
      "Epoch: 3, Idx: 500, Training Loss: 0.6937, Training Accuracy:  56.25%\n",
      "Epoch: 03, Train Loss: 0.685, Train Acc: 60.74%, Val. Loss: 0.728285, Val. Acc: 54.92%\n",
      "Epoch: 4, Idx: 100, Training Loss: 0.7239, Training Accuracy:  59.38%\n",
      "Epoch: 4, Idx: 200, Training Loss: 0.5938, Training Accuracy:  75.00%\n",
      "Epoch: 4, Idx: 300, Training Loss: 0.9016, Training Accuracy:  53.12%\n",
      "Epoch: 4, Idx: 400, Training Loss: 0.5571, Training Accuracy:  78.12%\n",
      "Epoch: 4, Idx: 500, Training Loss: 0.5879, Training Accuracy:  65.62%\n",
      "Epoch: 04, Train Loss: 0.667, Train Acc: 61.56%, Val. Loss: 0.678201, Val. Acc: 57.76%\n",
      "Epoch: 5, Idx: 100, Training Loss: 0.6851, Training Accuracy:  56.25%\n",
      "Epoch: 5, Idx: 200, Training Loss: 0.6559, Training Accuracy:  62.50%\n",
      "Epoch: 5, Idx: 300, Training Loss: 0.6128, Training Accuracy:  62.50%\n",
      "Epoch: 5, Idx: 400, Training Loss: 0.6151, Training Accuracy:  59.38%\n",
      "Epoch: 5, Idx: 500, Training Loss: 0.6839, Training Accuracy:  68.75%\n",
      "Epoch: 05, Train Loss: 0.669, Train Acc: 61.76%, Val. Loss: 0.631271, Val. Acc: 66.40%\n",
      "Epoch: 6, Idx: 100, Training Loss: 0.4968, Training Accuracy:  78.12%\n",
      "Epoch: 6, Idx: 200, Training Loss: 0.7118, Training Accuracy:  62.50%\n",
      "Epoch: 6, Idx: 300, Training Loss: 0.5181, Training Accuracy:  81.25%\n",
      "Epoch: 6, Idx: 400, Training Loss: 0.5818, Training Accuracy:  75.00%\n",
      "Epoch: 6, Idx: 500, Training Loss: 0.5787, Training Accuracy:  68.75%\n",
      "Epoch: 06, Train Loss: 0.664, Train Acc: 62.54%, Val. Loss: 0.714283, Val. Acc: 53.20%\n",
      "Epoch: 7, Idx: 100, Training Loss: 0.7741, Training Accuracy:  65.62%\n",
      "Epoch: 7, Idx: 200, Training Loss: 0.6719, Training Accuracy:  62.50%\n",
      "Epoch: 7, Idx: 300, Training Loss: 0.5993, Training Accuracy:  68.75%\n",
      "Epoch: 7, Idx: 400, Training Loss: 0.7759, Training Accuracy:  46.88%\n",
      "Epoch: 7, Idx: 500, Training Loss: 0.6450, Training Accuracy:  59.38%\n",
      "Epoch: 07, Train Loss: 0.659, Train Acc: 62.85%, Val. Loss: 0.643714, Val. Acc: 61.96%\n",
      "Epoch: 8, Idx: 100, Training Loss: 0.6427, Training Accuracy:  75.00%\n",
      "Epoch: 8, Idx: 200, Training Loss: 0.7509, Training Accuracy:  46.88%\n",
      "Epoch: 8, Idx: 300, Training Loss: 0.7016, Training Accuracy:  53.12%\n",
      "Epoch: 8, Idx: 400, Training Loss: 0.6085, Training Accuracy:  71.88%\n",
      "Epoch: 8, Idx: 500, Training Loss: 0.5723, Training Accuracy:  71.88%\n",
      "Epoch: 08, Train Loss: 0.661, Train Acc: 63.19%, Val. Loss: 0.631669, Val. Acc: 66.58%\n",
      "Epoch: 9, Idx: 100, Training Loss: 0.6699, Training Accuracy:  56.25%\n",
      "Epoch: 9, Idx: 200, Training Loss: 0.7980, Training Accuracy:  56.25%\n",
      "Epoch: 9, Idx: 300, Training Loss: 0.7833, Training Accuracy:  56.25%\n",
      "Epoch: 9, Idx: 400, Training Loss: 0.6437, Training Accuracy:  56.25%\n",
      "Epoch: 9, Idx: 500, Training Loss: 0.6734, Training Accuracy:  65.62%\n",
      "Epoch: 09, Train Loss: 0.658, Train Acc: 63.68%, Val. Loss: 0.675087, Val. Acc: 61.79%\n",
      "Epoch: 10, Idx: 100, Training Loss: 0.5863, Training Accuracy:  75.00%\n",
      "Epoch: 10, Idx: 200, Training Loss: 0.6492, Training Accuracy:  71.88%\n",
      "Epoch: 10, Idx: 300, Training Loss: 0.5064, Training Accuracy:  84.38%\n",
      "Epoch: 10, Idx: 400, Training Loss: 0.8142, Training Accuracy:  53.12%\n",
      "Epoch: 10, Idx: 500, Training Loss: 0.7308, Training Accuracy:  46.88%\n",
      "Epoch: 10, Train Loss: 0.651, Train Acc: 64.40%, Val. Loss: 0.645745, Val. Acc: 65.99%\n",
      "Test Loss: 0.647, Test Acc: 65.61%\n"
     ]
    }
   ],
   "source": [
    "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_dataset()\n",
    "\n",
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "    \n",
    "def train_model(model, train_iter, epoch):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    model.cuda()\n",
    "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        text = batch.text[0]\n",
    "        target = batch.label\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        if torch.cuda.is_available():\n",
    "            text = text.cuda()\n",
    "            target = target.cuda()\n",
    "        if (text.size()[0] is not 32):# One of the batch returned by BucketIterator has length different than 32.\n",
    "            continue\n",
    "        optim.zero_grad()\n",
    "        prediction = model(text)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "        acc = 100.0 * num_corrects/len(batch)\n",
    "        loss.backward()\n",
    "        clip_gradient(model, 1e-1)\n",
    "        optim.step()\n",
    "        steps += 1\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
    "        \n",
    "        total_epoch_loss += loss.item()\n",
    "        total_epoch_acc += acc.item()\n",
    "        \n",
    "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n",
    "\n",
    "def eval_model(model, val_iter):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_iter):\n",
    "            text = batch.text[0]\n",
    "            if (text.size()[0] is not 32):\n",
    "                continue\n",
    "            target = batch.label\n",
    "            target = torch.autograd.Variable(target).long()\n",
    "            if torch.cuda.is_available():\n",
    "                text = text.cuda()\n",
    "                target = target.cuda()\n",
    "            prediction = model(text)\n",
    "            loss = loss_fn(prediction, target)\n",
    "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "\n",
    "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)\n",
    "\n",
    "learning_rate = 1e-5\n",
    "batch_size = 32\n",
    "output_size = 2\n",
    "hidden_size = 256\n",
    "embedding_length = 300\n",
    "\n",
    "model = RNN(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
    "    val_loss, val_acc = eval_model(model, valid_iter)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
    "    \n",
    "test_loss, test_acc = eval_model(model, test_iter)\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1081, 0.8919]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Sentiment: Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justin\\venv\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test_sen1 = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n",
    "\n",
    "test_sen1 = TEXT.preprocess(test_sen1)\n",
    "test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n",
    "\n",
    "test_sen = np.asarray(test_sen1)\n",
    "test_sen = torch.LongTensor(test_sen)\n",
    "test_tensor = Variable(test_sen, volatile=True)\n",
    "test_tensor = test_tensor.cuda()\n",
    "model.eval()\n",
    "output = model(test_tensor, 1)\n",
    "out = F.softmax(output, 1)\n",
    "print(out)\n",
    "if (torch.argmax(out[0]) == 1):\n",
    "    print (\"Sentiment: Positive\")\n",
    "else:\n",
    "    print (\"Sentiment: Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6741, 0.3259]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Sentiment: Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justin\\venv\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "test_sen2 = \"Ohh, such a ridiculous movie. Not gonna recommend it to anyone. Complete waste of time and money.\"\n",
    "test_sen2 = TEXT.preprocess(test_sen2)\n",
    "test_sen2 = [[TEXT.vocab.stoi[x] for x in test_sen2]]\n",
    "\n",
    "test_sen = np.asarray(test_sen2)\n",
    "test_sen = torch.LongTensor(test_sen)\n",
    "test_tensor = Variable(test_sen, volatile=True)\n",
    "test_tensor = test_tensor.cuda()\n",
    "model.eval()\n",
    "output = model(test_tensor, 1)\n",
    "out = F.softmax(output, 1)\n",
    "print(out)\n",
    "if (torch.argmax(out[0]) == 1):\n",
    "    print (\"Sentiment: Positive\")\n",
    "else:\n",
    "    print (\"Sentiment: Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Idx: 100, Training Loss: 0.6934, Training Accuracy:  50.00%\n",
      "Epoch: 1, Idx: 200, Training Loss: 0.7078, Training Accuracy:  46.88%\n",
      "Epoch: 1, Idx: 300, Training Loss: 0.6930, Training Accuracy:  56.25%\n",
      "Epoch: 1, Idx: 400, Training Loss: 0.6874, Training Accuracy:  56.25%\n",
      "Epoch: 1, Idx: 500, Training Loss: 0.6718, Training Accuracy:  78.12%\n",
      "Epoch: 01, Train Loss: 0.690, Train Acc: 51.97%, Val. Loss: 0.688056, Val. Acc: 51.27%\n",
      "Epoch: 2, Idx: 100, Training Loss: 0.6545, Training Accuracy:  62.50%\n",
      "Epoch: 2, Idx: 200, Training Loss: 0.6695, Training Accuracy:  62.50%\n",
      "Epoch: 2, Idx: 300, Training Loss: 0.6926, Training Accuracy:  53.12%\n",
      "Epoch: 2, Idx: 400, Training Loss: 0.6827, Training Accuracy:  56.25%\n",
      "Epoch: 2, Idx: 500, Training Loss: 0.6129, Training Accuracy:  68.75%\n",
      "Epoch: 02, Train Loss: 0.667, Train Acc: 58.72%, Val. Loss: 0.682012, Val. Acc: 54.80%\n",
      "Epoch: 3, Idx: 100, Training Loss: 0.6308, Training Accuracy:  68.75%\n",
      "Epoch: 3, Idx: 200, Training Loss: 0.4342, Training Accuracy:  84.38%\n",
      "Epoch: 3, Idx: 300, Training Loss: 0.6503, Training Accuracy:  62.50%\n",
      "Epoch: 3, Idx: 400, Training Loss: 0.6636, Training Accuracy:  68.75%\n",
      "Epoch: 3, Idx: 500, Training Loss: 0.5156, Training Accuracy:  75.00%\n",
      "Epoch: 03, Train Loss: 0.590, Train Acc: 69.50%, Val. Loss: 0.494978, Val. Acc: 77.09%\n",
      "Epoch: 4, Idx: 100, Training Loss: 0.5605, Training Accuracy:  71.88%\n",
      "Epoch: 4, Idx: 200, Training Loss: 0.8281, Training Accuracy:  65.62%\n",
      "Epoch: 4, Idx: 300, Training Loss: 0.6036, Training Accuracy:  65.62%\n",
      "Epoch: 4, Idx: 400, Training Loss: 0.4735, Training Accuracy:  71.88%\n",
      "Epoch: 4, Idx: 500, Training Loss: 0.4546, Training Accuracy:  78.12%\n",
      "Epoch: 04, Train Loss: 0.428, Train Acc: 80.67%, Val. Loss: 0.386465, Val. Acc: 82.08%\n",
      "Epoch: 5, Idx: 100, Training Loss: 0.3328, Training Accuracy:  87.50%\n",
      "Epoch: 5, Idx: 200, Training Loss: 0.3596, Training Accuracy:  78.12%\n",
      "Epoch: 5, Idx: 300, Training Loss: 0.3249, Training Accuracy:  87.50%\n",
      "Epoch: 5, Idx: 400, Training Loss: 0.6565, Training Accuracy:  68.75%\n",
      "Epoch: 5, Idx: 500, Training Loss: 0.4050, Training Accuracy:  78.12%\n",
      "Epoch: 05, Train Loss: 0.367, Train Acc: 83.72%, Val. Loss: 0.369900, Val. Acc: 82.79%\n",
      "Epoch: 6, Idx: 100, Training Loss: 0.4549, Training Accuracy:  84.38%\n",
      "Epoch: 6, Idx: 200, Training Loss: 0.3892, Training Accuracy:  81.25%\n",
      "Epoch: 6, Idx: 300, Training Loss: 0.1442, Training Accuracy:  96.88%\n",
      "Epoch: 6, Idx: 400, Training Loss: 0.3001, Training Accuracy:  87.50%\n",
      "Epoch: 6, Idx: 500, Training Loss: 0.4553, Training Accuracy:  75.00%\n",
      "Epoch: 06, Train Loss: 0.324, Train Acc: 85.52%, Val. Loss: 0.367029, Val. Acc: 83.53%\n",
      "Epoch: 7, Idx: 100, Training Loss: 0.2308, Training Accuracy:  90.62%\n",
      "Epoch: 7, Idx: 200, Training Loss: 0.3394, Training Accuracy:  81.25%\n",
      "Epoch: 7, Idx: 300, Training Loss: 0.4261, Training Accuracy:  87.50%\n",
      "Epoch: 7, Idx: 400, Training Loss: 0.3106, Training Accuracy:  90.62%\n",
      "Epoch: 7, Idx: 500, Training Loss: 0.1421, Training Accuracy:  96.88%\n",
      "Epoch: 07, Train Loss: 0.282, Train Acc: 87.91%, Val. Loss: 0.378974, Val. Acc: 83.80%\n",
      "Epoch: 8, Idx: 100, Training Loss: 0.1280, Training Accuracy:  96.88%\n",
      "Epoch: 8, Idx: 200, Training Loss: 0.4244, Training Accuracy:  84.38%\n",
      "Epoch: 8, Idx: 300, Training Loss: 0.3225, Training Accuracy:  90.62%\n",
      "Epoch: 8, Idx: 400, Training Loss: 0.3618, Training Accuracy:  84.38%\n",
      "Epoch: 8, Idx: 500, Training Loss: 0.2334, Training Accuracy:  87.50%\n",
      "Epoch: 08, Train Loss: 0.232, Train Acc: 90.26%, Val. Loss: 0.395538, Val. Acc: 83.34%\n",
      "Epoch: 9, Idx: 100, Training Loss: 0.1379, Training Accuracy:  96.88%\n",
      "Epoch: 9, Idx: 200, Training Loss: 0.2220, Training Accuracy:  87.50%\n",
      "Epoch: 9, Idx: 300, Training Loss: 0.2743, Training Accuracy:  87.50%\n",
      "Epoch: 9, Idx: 400, Training Loss: 0.3071, Training Accuracy:  84.38%\n",
      "Epoch: 9, Idx: 500, Training Loss: 0.1465, Training Accuracy:  93.75%\n",
      "Epoch: 09, Train Loss: 0.181, Train Acc: 92.97%, Val. Loss: 0.433728, Val. Acc: 83.30%\n",
      "Epoch: 10, Idx: 100, Training Loss: 0.0535, Training Accuracy:  96.88%\n",
      "Epoch: 10, Idx: 200, Training Loss: 0.1756, Training Accuracy:  90.62%\n",
      "Epoch: 10, Idx: 300, Training Loss: 0.0970, Training Accuracy:  100.00%\n",
      "Epoch: 10, Idx: 400, Training Loss: 0.1835, Training Accuracy:  93.75%\n",
      "Epoch: 10, Idx: 500, Training Loss: 0.0648, Training Accuracy:  96.88%\n",
      "Epoch: 10, Train Loss: 0.133, Train Acc: 94.96%, Val. Loss: 0.458091, Val. Acc: 82.26%\n",
      "Test Loss: 0.456, Test Acc: 82.61%\n"
     ]
    }
   ],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)# Initializing the look-up table.\n",
    "        self.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assigning the look-up table to the pre-trained GloVe word embedding.\n",
    "        self.lstm = nn.LSTM(embedding_length, hidden_size)\n",
    "        self.label = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_sentence, batch_size=None):\n",
    "        input = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences,  embedding_length)\n",
    "        input = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n",
    "        if batch_size is None:\n",
    "            h_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial hidden state of the LSTM\n",
    "            c_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial cell state of the LSTM\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
    "            c_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
    "        final_output = self.label(final_hidden_state[-1]) # final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\n",
    "        return final_output\n",
    "    \n",
    "model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
    "    val_loss, val_acc = eval_model(model, valid_iter)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
    "    \n",
    "test_loss, test_acc = eval_model(model, test_iter)\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.6929e-06, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Sentiment: Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justin\\venv\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test_sen1 = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n",
    "\n",
    "test_sen1 = TEXT.preprocess(test_sen1)\n",
    "test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n",
    "\n",
    "test_sen = np.asarray(test_sen1)\n",
    "test_sen = torch.LongTensor(test_sen)\n",
    "test_tensor = Variable(test_sen, volatile=True)\n",
    "test_tensor = test_tensor.cuda()\n",
    "model.eval()\n",
    "output = model(test_tensor, 1)\n",
    "out = F.softmax(output, 1)\n",
    "print(out)\n",
    "if (torch.argmax(out[0]) == 1):\n",
    "    print (\"Sentiment: Positive\")\n",
    "else:\n",
    "    print (\"Sentiment: Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9989, 0.0011]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Sentiment: Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justin\\venv\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test_sen2 = \"Ohh, such a ridiculous movie. Not gonna recommend it to anyone. Complete waste of time and money.\"\n",
    "test_sen2 = TEXT.preprocess(test_sen2)\n",
    "test_sen2 = [[TEXT.vocab.stoi[x] for x in test_sen2]]\n",
    "\n",
    "\n",
    "test_sen = np.asarray(test_sen2)\n",
    "test_sen = torch.LongTensor(test_sen)\n",
    "test_tensor = Variable(test_sen, volatile=True)\n",
    "test_tensor = test_tensor.cuda()\n",
    "model.eval()\n",
    "output = model(test_tensor, 1)\n",
    "out = F.softmax(output, 1)\n",
    "print(out)\n",
    "if (torch.argmax(out[0]) == 1):\n",
    "    print (\"Sentiment: Positive\")\n",
    "else:\n",
    "    print (\"Sentiment: Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModel(torch.nn.Module):\n",
    "    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
    "        super(AttentionModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
    "        self.lstm = nn.LSTM(embedding_length, hidden_size)\n",
    "        self.label = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        hidden = final_state.squeeze(0)\n",
    "\n",
    "        attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        \n",
    "        return new_hidden_state\n",
    "    \n",
    "    def forward(self, input_sentences, batch_size=None):\n",
    "        input = self.word_embeddings(input_sentences)\n",
    "\n",
    "        input = input.permute(1, 0, 2)\n",
    "        if batch_size is None:\n",
    "            h_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda())\n",
    "            c_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda())\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
    "            c_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
    "\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
    "        output = output.permute(1, 0, 2)\n",
    "        \n",
    "        attn_output = self.attention_net(output, final_hidden_state)\n",
    "\n",
    "        logits = self.label(attn_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Idx: 100, Training Loss: 0.7107, Training Accuracy:  46.88%\n",
      "Epoch: 1, Idx: 200, Training Loss: 0.6477, Training Accuracy:  65.62%\n",
      "Epoch: 1, Idx: 300, Training Loss: 0.6709, Training Accuracy:  59.38%\n",
      "Epoch: 1, Idx: 400, Training Loss: 0.6348, Training Accuracy:  71.88%\n",
      "Epoch: 1, Idx: 500, Training Loss: 0.6457, Training Accuracy:  71.88%\n",
      "Epoch: 01, Train Loss: 0.670, Train Acc: 61.15%, Val. Loss: 0.622058, Val. Acc: 69.28%\n",
      "Epoch: 2, Idx: 100, Training Loss: 0.5689, Training Accuracy:  81.25%\n",
      "Epoch: 2, Idx: 200, Training Loss: 0.7160, Training Accuracy:  59.38%\n",
      "Epoch: 2, Idx: 300, Training Loss: 0.5420, Training Accuracy:  78.12%\n",
      "Epoch: 2, Idx: 400, Training Loss: 0.4943, Training Accuracy:  78.12%\n",
      "Epoch: 2, Idx: 500, Training Loss: 0.4309, Training Accuracy:  87.50%\n",
      "Epoch: 02, Train Loss: 0.517, Train Acc: 76.50%, Val. Loss: 0.500942, Val. Acc: 76.73%\n",
      "Epoch: 3, Idx: 100, Training Loss: 0.4180, Training Accuracy:  78.12%\n",
      "Epoch: 3, Idx: 200, Training Loss: 0.1869, Training Accuracy:  93.75%\n",
      "Epoch: 3, Idx: 300, Training Loss: 0.4827, Training Accuracy:  75.00%\n",
      "Epoch: 3, Idx: 400, Training Loss: 0.3836, Training Accuracy:  84.38%\n",
      "Epoch: 3, Idx: 500, Training Loss: 0.6573, Training Accuracy:  68.75%\n",
      "Epoch: 03, Train Loss: 0.315, Train Acc: 87.20%, Val. Loss: 0.440898, Val. Acc: 79.07%\n",
      "Epoch: 4, Idx: 100, Training Loss: 0.1041, Training Accuracy:  96.88%\n",
      "Epoch: 4, Idx: 200, Training Loss: 0.0691, Training Accuracy:  100.00%\n",
      "Epoch: 4, Idx: 300, Training Loss: 0.0704, Training Accuracy:  96.88%\n",
      "Epoch: 4, Idx: 400, Training Loss: 0.1435, Training Accuracy:  93.75%\n",
      "Epoch: 4, Idx: 500, Training Loss: 0.1228, Training Accuracy:  96.88%\n",
      "Epoch: 04, Train Loss: 0.163, Train Acc: 93.80%, Val. Loss: 0.498487, Val. Acc: 81.04%\n",
      "Epoch: 5, Idx: 100, Training Loss: 0.0418, Training Accuracy:  96.88%\n",
      "Epoch: 5, Idx: 200, Training Loss: 0.0405, Training Accuracy:  96.88%\n",
      "Epoch: 5, Idx: 300, Training Loss: 0.1384, Training Accuracy:  90.62%\n",
      "Epoch: 5, Idx: 400, Training Loss: 0.2633, Training Accuracy:  90.62%\n",
      "Epoch: 5, Idx: 500, Training Loss: 0.0360, Training Accuracy:  100.00%\n",
      "Epoch: 05, Train Loss: 0.079, Train Acc: 97.15%, Val. Loss: 0.572422, Val. Acc: 81.11%\n",
      "Epoch: 6, Idx: 100, Training Loss: 0.0018, Training Accuracy:  100.00%\n",
      "Epoch: 6, Idx: 200, Training Loss: 0.0444, Training Accuracy:  96.88%\n",
      "Epoch: 6, Idx: 300, Training Loss: 0.1177, Training Accuracy:  96.88%\n",
      "Epoch: 6, Idx: 400, Training Loss: 0.1992, Training Accuracy:  96.88%\n",
      "Epoch: 6, Idx: 500, Training Loss: 0.0245, Training Accuracy:  100.00%\n",
      "Epoch: 06, Train Loss: 0.034, Train Acc: 98.77%, Val. Loss: 0.777436, Val. Acc: 81.54%\n",
      "Epoch: 7, Idx: 100, Training Loss: 0.0051, Training Accuracy:  100.00%\n",
      "Epoch: 7, Idx: 200, Training Loss: 0.0011, Training Accuracy:  100.00%\n",
      "Epoch: 7, Idx: 300, Training Loss: 0.0009, Training Accuracy:  100.00%\n",
      "Epoch: 7, Idx: 400, Training Loss: 0.0038, Training Accuracy:  100.00%\n",
      "Epoch: 7, Idx: 500, Training Loss: 0.0092, Training Accuracy:  100.00%\n",
      "Epoch: 07, Train Loss: 0.021, Train Acc: 99.27%, Val. Loss: 0.816149, Val. Acc: 81.73%\n",
      "Epoch: 8, Idx: 100, Training Loss: 0.0013, Training Accuracy:  100.00%\n",
      "Epoch: 8, Idx: 200, Training Loss: 0.0011, Training Accuracy:  100.00%\n",
      "Epoch: 8, Idx: 300, Training Loss: 0.0069, Training Accuracy:  100.00%\n",
      "Epoch: 8, Idx: 400, Training Loss: 0.0105, Training Accuracy:  100.00%\n",
      "Epoch: 8, Idx: 500, Training Loss: 0.0003, Training Accuracy:  100.00%\n",
      "Epoch: 08, Train Loss: 0.016, Train Acc: 99.34%, Val. Loss: 0.735093, Val. Acc: 80.49%\n",
      "Epoch: 9, Idx: 100, Training Loss: 0.0003, Training Accuracy:  100.00%\n",
      "Epoch: 9, Idx: 200, Training Loss: 0.0004, Training Accuracy:  100.00%\n",
      "Epoch: 9, Idx: 300, Training Loss: 0.0004, Training Accuracy:  100.00%\n",
      "Epoch: 9, Idx: 400, Training Loss: 0.0025, Training Accuracy:  100.00%\n",
      "Epoch: 9, Idx: 500, Training Loss: 0.0021, Training Accuracy:  100.00%\n",
      "Epoch: 09, Train Loss: 0.008, Train Acc: 99.58%, Val. Loss: 1.018735, Val. Acc: 81.88%\n",
      "Epoch: 10, Idx: 100, Training Loss: 0.0025, Training Accuracy:  100.00%\n",
      "Epoch: 10, Idx: 200, Training Loss: 0.0192, Training Accuracy:  100.00%\n",
      "Epoch: 10, Idx: 300, Training Loss: 0.0002, Training Accuracy:  100.00%\n",
      "Epoch: 10, Idx: 400, Training Loss: 0.0000, Training Accuracy:  100.00%\n",
      "Epoch: 10, Idx: 500, Training Loss: 0.0082, Training Accuracy:  100.00%\n",
      "Epoch: 10, Train Loss: 0.011, Train Acc: 99.55%, Val. Loss: 0.810989, Val. Acc: 82.54%\n",
      "Test Loss: 0.940, Test Acc: 80.09%\n"
     ]
    }
   ],
   "source": [
    "model = AttentionModel(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
    "    val_loss, val_acc = eval_model(model, valid_iter)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
    "    \n",
    "test_loss, test_acc = eval_model(model, test_iter)\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.2883e-07, 1.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Sentiment: Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justin\\venv\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test_sen1 = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n",
    "\n",
    "test_sen1 = TEXT.preprocess(test_sen1)\n",
    "test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n",
    "\n",
    "test_sen = np.asarray(test_sen1)\n",
    "test_sen = torch.LongTensor(test_sen)\n",
    "test_tensor = Variable(test_sen, volatile=True)\n",
    "test_tensor = test_tensor.cuda()\n",
    "model.eval()\n",
    "output = model(test_tensor, 1)\n",
    "out = F.softmax(output, 1)\n",
    "print(out)\n",
    "if (torch.argmax(out[0]) == 1):\n",
    "    print (\"Sentiment: Positive\")\n",
    "else:\n",
    "    print (\"Sentiment: Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+00, 1.0964e-06]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Sentiment: Negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justin\\venv\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test_sen2 = \"Ohh, such a ridiculous movie. Not gonna recommend it to anyone. Complete waste of time and money.\"\n",
    "test_sen2 = TEXT.preprocess(test_sen2)\n",
    "test_sen2 = [[TEXT.vocab.stoi[x] for x in test_sen2]]\n",
    "\n",
    "\n",
    "test_sen = np.asarray(test_sen2)\n",
    "test_sen = torch.LongTensor(test_sen)\n",
    "test_tensor = Variable(test_sen, volatile=True)\n",
    "test_tensor = test_tensor.cuda()\n",
    "model.eval()\n",
    "output = model(test_tensor, 1)\n",
    "out = F.softmax(output, 1)\n",
    "print(out)\n",
    "if (torch.argmax(out[0]) == 1):\n",
    "    print (\"Sentiment: Positive\")\n",
    "else:\n",
    "    print (\"Sentiment: Negative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
